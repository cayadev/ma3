{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part I: RAG\n",
    "\n",
    "Please see the description of the assignment in the README file (section 1) <br>\n",
    "**Guide notebook**: [guides/rag_guide.ipynb](guides/rag_guide.ipynb)\n",
    "\n",
    "\n",
    "***\n",
    "<br>\n",
    "\n",
    "* Remember to include some reflections on your results. Are there, for example, any hyperparameters that are particularly important?\n",
    "\n",
    "* You should follow the steps given in the `rag_guide` notebook to create your own RAG system.\n",
    "\n",
    "<br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Any\n",
    "from copy import deepcopy\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from decouple import config\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import Image, display\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters.markdown import MarkdownHeaderTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from langgraph.graph import START, StateGraph\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "import litellm\n",
    "from litellm import completion\n",
    "import instructor\n",
    "from instructor import Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Ensure this runs before accessing environment variables\n",
    "\n",
    "WX_API_KEY = os.getenv(\"WX_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "WX_API_KEY = config(\"WX_API_KEY\")\n",
    "WX_PROJECT_ID = config(\"WX_PROJECT_ID\")\n",
    "WX_API_URL = \"https://us-south.ml.cloud.ibm.com\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authenticate and initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = WatsonxLLM(\n",
    "\n",
    "        model_id= \"ibm/granite-3-8b-instruct\",\n",
    "        url=WX_API_URL,\n",
    "        apikey=WX_API_KEY,\n",
    "        project_id=WX_PROJECT_ID,\n",
    "\n",
    "        params={\n",
    "            GenParams.DECODING_METHOD: \"greedy\", \n",
    "            GenParams.TEMPERATURE: 0,\n",
    "            GenParams.MIN_NEW_TOKENS: 5,\n",
    "            GenParams.MAX_NEW_TOKENS: 1_000,\n",
    "            GenParams.REPETITION_PENALTY:1.2\n",
    "        }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.outputs.llm_result.LLMResult'>\n",
      "generations=[[Generation(text=\"\\nI'm an artificial intelligence and don't have feelings, but I'm here to help you. How can I assist you today?\", generation_info={'finish_reason': 'eos_token'})]] llm_output={'token_usage': {'generated_token_count': 31, 'input_token_count': 5}, 'model_id': 'ibm/granite-3-8b-instruct', 'deployment_id': None} run=[RunInfo(run_id=UUID('c709b894-3c36-41b6-a8e3-748c89ccb98c'))] type='LLMResult'\n"
     ]
    }
   ],
   "source": [
    "llm_result = llm.generate([\"Hi how are you?\"])\n",
    "\n",
    "print(type(llm_result))\n",
    "print(llm_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load documents\n",
    "\n",
    "load text documents from our local system to build a knowledge base. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data/madeup_company.md'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = TextLoader(\"data/madeup_company.md\").load()[0]\n",
    "document.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split documents\n",
    "\n",
    "Since we are dealing with a markdown file, we can use `MarkdownHeaderTextSplitter`. This splitter will split the document into chunks based on the headers in the markdown file. This is a good way to maintain the structure of the document and ensure that the chunks are coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [(\"#\", \"Header 1\"), (\"##\", \"Header 2\"), (\"###\", \"Header 3\"), (\"####\", \"Header 4\")]\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "chunks = text_splitter.split_text(document.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess chunks\n",
    "\n",
    "If we looks at the metadata for each chunk, we see that it is neatly split into header types (Header 1, header 2, etc.) and the text content. This is useful! We can write a function to add the header type(s) to the text content itself, so that the model can use this information to generate better responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_documents_with_headers(chunks):\n",
    "    \"\"\"\n",
    "    Creates a new list of Document objects with page_content prepended with headers\n",
    "    in [Header1/Header2/Header3]: format\n",
    "    \n",
    "    Returns new objects rather than modifying the original chunks\n",
    "    \"\"\"\n",
    "    updated_chunks = []\n",
    "    \n",
    "    for doc in chunks:\n",
    "        # Create a deep copy of the document to avoid modifying the original\n",
    "        new_doc = deepcopy(doc)\n",
    "        \n",
    "        # Get all headers that exist in metadata\n",
    "        headers = []\n",
    "        for i in range(1, 4):\n",
    "            key = f'Header {i}'\n",
    "            if key in new_doc.metadata:\n",
    "                headers.append(new_doc.metadata[key])\n",
    "        \n",
    "        # Create the header prefix and update page_content\n",
    "        if headers:\n",
    "            prefix = f\"[{'/'.join(headers)}]: \"\n",
    "            new_doc.page_content = prefix + \"\\n\" + new_doc.page_content\n",
    "        \n",
    "        updated_chunks.append(new_doc)\n",
    "    \n",
    "    return updated_chunks\n",
    "\n",
    "\n",
    "docs = update_documents_with_headers(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the embedding model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_params = {}\n",
    "\n",
    "watsonx_embedding = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/granite-embedding-278m-multilingual\",\n",
    "    url=WX_API_URL,\n",
    "    project_id=WX_PROJECT_ID,\n",
    "    apikey=WX_API_KEY,\n",
    "    params=embed_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_vector_db = Chroma.from_documents(\n",
    "    collection_name=\"my_collection\",\n",
    "    embedding=watsonx_embedding,\n",
    "    persist_directory=\"my_vector_db\", # This will save the vector database to disk! Delete it if you want to start fresh.\n",
    "    documents=docs,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve documents with semantic search\n",
    "\n",
    "In LangChain we use `VectorStoreRetriever` as a sort of wrapper around the vector index. This retriever is used to search for documents based on their embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the vectorstore as a retriever\n",
    "retriever = local_vector_db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 3,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a RAG prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context: \n",
    "{context} \n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining our RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is CloudMate?\"\n",
    "\n",
    "retrieved_docs = local_vector_db.similarity_search(question)\n",
    "docs_content = \"\\n\\n\".join(f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(retrieved_docs))\n",
    "formated_prompt = prompt.invoke({\"question\": question, \"context\": docs_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "\n",
      "Question:\n",
      "What is CloudMate?\n",
      "\n",
      "Context: \n",
      "Document 1:\n",
      "[About MadeUpCompany/Products and Services/CloudMate – Secure and Scalable Cloud Storage]: \n",
      "CloudMate is our flagship cloud storage solution, designed for businesses of all sizes. Features include:\n",
      "- ✅ Seamless data migration with automated backups\n",
      "- ✅ Military-grade encryption and multi-factor authentication\n",
      "- ✅ Role-based access control for enterprise security\n",
      "- ✅ AI-powered file organization and search capabilities\n",
      "\n",
      "Document 2:\n",
      "[About MadeUpCompany/Pricing/CloudMate Plans]: \n",
      "Our secure and scalable cloud storage service, CloudMate, is available in the following plans:\n",
      "- Basic: $9.99/month – 100GB storage, essential security features\n",
      "- Professional: $29.99/month – 1TB storage, enhanced security, pr\n"
     ]
    }
   ],
   "source": [
    "print(formated_prompt.to_string()[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm.invoke(formated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CloudMate is MadeUpCompany's flagship cloud storage solution, offering seamless data migration, military-grade encryption, role-based access control, and AI-powered file organization among its key features. It comes in different plans catering to varying business needs, ranging from basic to enterprise levels.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
